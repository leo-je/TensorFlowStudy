{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a3b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.0.0 \tGPU: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 00:00:25.156933: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tensorflow_version = tf.__version__\n",
    "\n",
    "gpu_availabe = tf.test.is_gpu_available()\n",
    "\n",
    "print(\"tensorflow version:\",tensorflow_version,\"\\tGPU:\",gpu_availabe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ff691f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 4.], shape=(2,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 00:00:25.179281: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1.0,2.0],name=\"a\")\n",
    "b = tf.constant([1.0,2.0],name=\"b\")\n",
    "result = tf.add(a,b,name=\"add\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0edd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epoch,w is 2.600000,loss is 36.000000\n",
      "After 1 epoch,w is 1.160000,loss is 12.959999\n",
      "After 2 epoch,w is 0.296000,loss is 4.665599\n",
      "After 3 epoch,w is -0.222400,loss is 1.679616\n",
      "After 4 epoch,w is -0.533440,loss is 0.604662\n",
      "After 5 epoch,w is -0.720064,loss is 0.217678\n",
      "After 6 epoch,w is -0.832038,loss is 0.078364\n",
      "After 7 epoch,w is -0.899223,loss is 0.028211\n",
      "After 8 epoch,w is -0.939534,loss is 0.010156\n",
      "After 9 epoch,w is -0.963720,loss is 0.003656\n",
      "After 10 epoch,w is -0.978232,loss is 0.001316\n",
      "After 11 epoch,w is -0.986939,loss is 0.000474\n",
      "After 12 epoch,w is -0.992164,loss is 0.000171\n",
      "After 13 epoch,w is -0.995298,loss is 0.000061\n",
      "After 14 epoch,w is -0.997179,loss is 0.000022\n",
      "After 15 epoch,w is -0.998307,loss is 0.000008\n",
      "After 16 epoch,w is -0.998984,loss is 0.000003\n",
      "After 17 epoch,w is -0.999391,loss is 0.000001\n",
      "After 18 epoch,w is -0.999634,loss is 0.000000\n",
      "After 19 epoch,w is -0.999781,loss is 0.000000\n",
      "After 20 epoch,w is -0.999868,loss is 0.000000\n",
      "After 21 epoch,w is -0.999921,loss is 0.000000\n",
      "After 22 epoch,w is -0.999953,loss is 0.000000\n",
      "After 23 epoch,w is -0.999972,loss is 0.000000\n",
      "After 24 epoch,w is -0.999983,loss is 0.000000\n",
      "After 25 epoch,w is -0.999990,loss is 0.000000\n",
      "After 26 epoch,w is -0.999994,loss is 0.000000\n",
      "After 27 epoch,w is -0.999996,loss is 0.000000\n",
      "After 28 epoch,w is -0.999998,loss is 0.000000\n",
      "After 29 epoch,w is -0.999999,loss is 0.000000\n",
      "After 30 epoch,w is -0.999999,loss is 0.000000\n",
      "After 31 epoch,w is -1.000000,loss is 0.000000\n",
      "After 32 epoch,w is -1.000000,loss is 0.000000\n",
      "After 33 epoch,w is -1.000000,loss is 0.000000\n",
      "After 34 epoch,w is -1.000000,loss is 0.000000\n",
      "After 35 epoch,w is -1.000000,loss is 0.000000\n",
      "After 36 epoch,w is -1.000000,loss is 0.000000\n",
      "After 37 epoch,w is -1.000000,loss is 0.000000\n",
      "After 38 epoch,w is -1.000000,loss is 0.000000\n",
      "After 39 epoch,w is -1.000000,loss is 0.000000\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.constant(5,dtype=tf.float32))\n",
    "# 学习率\n",
    "lr = 0.2\n",
    "# 循环次数\n",
    "epoch = 40\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(w)\n",
    "        # 定义损失函数\n",
    "        loss = tf.square(w + 1)\n",
    "    grads = tape.gradient(loss,w)\n",
    "    \n",
    "    w.assign_sub(lr * grads)\n",
    "    print(\"After %s epoch,w is %f,loss is %f\" % (epoch,w.numpy(),loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c35df23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 5], shape=(2,), dtype=int64)\n",
      "<dtype: 'int64'>\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "# 张量 tensor\n",
    "a = tf.constant([1,5],dtype=tf.int64)\n",
    "print(a)\n",
    "print(a.dtype)\n",
    "print(a.shape) # shape=(2,) 表示 1维张量 y有两个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26c7cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(0,5)\n",
    "b = tf.convert_to_tensor(a,dtype=tf.int64)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b5266eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [2 2 3]], shape=(2, 3), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor([6 7], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#常用函数\n",
    "# 1.强制转换 tf.cast(张量名,dtype=数据类型)\n",
    "# 2.计算张量维度上元素的最小值 tf.reduce_min(张量名)\n",
    "# 3.计算张量维度上元素的最大值 tf.reduce_max(张量名)\n",
    "# 4.计算张量沿指定维度的平均值 tf.reduce_mean(张量名，axis=操作轴) 0为y方向 1为x方向\n",
    "# 5.计算张量沿指定维度的和 tf.reduce_sum(张量名，axis=操作轴)\n",
    "x1 = tf.constant([1.,2.,3.],dtype=tf.float64)\n",
    "print(x1)\n",
    "\n",
    "x2 = tf.cast(x1,tf.int32)\n",
    "print(x2)\n",
    "\n",
    "print(tf.reduce_min(x2),tf.reduce_max(x2))\n",
    "\n",
    "x = tf.constant([[1,2,3],[2,2,3]])\n",
    "print(x)\n",
    "print(tf.reduce_mean(x))\n",
    "print(tf.reduce_sum(x,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3135379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((), ()), types: (tf.int32, tf.int32)>\n",
      "(<tf.Tensor: id=490, shape=(), dtype=int32, numpy=12>, <tf.Tensor: id=491, shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: id=492, shape=(), dtype=int32, numpy=23>, <tf.Tensor: id=493, shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: id=494, shape=(), dtype=int32, numpy=10>, <tf.Tensor: id=495, shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: id=496, shape=(), dtype=int32, numpy=17>, <tf.Tensor: id=497, shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "# 5.tf.Variable(初始值) 将变量标记为可训练的，被标记的变量会在反向传播中记录梯度信息\n",
    "# 6. tf.add tf.subtract tf.multiply tf.divide 只有维度相同的张量才能进行四则运算\n",
    "# 7. tf.square 平方 tf.pow n次方 tf.sqrt 开平方根\n",
    "# 8. tf.matmul 矩阵乘\n",
    "# 9. tf.data.Dataset.from_tensor_slices 切分传入张量的第一维度，生成输入特征/标签对，构建数据集\n",
    "#     eg. tf.data.Dataset.from_tensor_slices((输入特征,标签))\n",
    "features = tf.constant([12,23,10,17])\n",
    "lables = tf.constant([0,1,1,0])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,lables))\n",
    "print(dataset)\n",
    "for e in dataset:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c3b863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 10. tf.GradientTape with结构记录计算过程，gradient求出张量的梯度\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    w = tf.Variable(tf.constant(3.0))\n",
    "    loss = tf.pow(w,2)\n",
    "grad = tape.gradient(loss,w)\n",
    "print(grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f599fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 one\n",
      "1 two\n"
     ]
    }
   ],
   "source": [
    "# 11. enumerate(列表名) python的内建函数，它可以遍历每个元素，组合为：索引 元素，常在for循环中使用\n",
    "seq = ['one','two']\n",
    "for index,item in enumerate(seq):\n",
    "    print(index,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c353af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 12. tf.one_hot(待转换数据，depth=几分类) 将待转换数据转换为one_hot形式的数据输出\n",
    "# 独热编码：在分类问题中，常用独热码做标签，标记类别： 1表示是，0表示非\n",
    "classes = 3\n",
    "lables = tf.constant([1,0,2]) #输入的元素最小值为0，最大值为2\n",
    "output = tf.one_hot(lables,depth=classes)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa0aa661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after softmax,y_pro is: tf.Tensor([0.25598174 0.69583046 0.04818781], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.nn.softmax 使输出符合概率分布\n",
    "\n",
    "y = tf.constant([1.01,2.01,-0.66])\n",
    "y_pro = tf.nn.softmax(y)\n",
    "print(\"after softmax,y_pro is:\",y_pro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34c6cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>\n"
     ]
    }
   ],
   "source": [
    "# 14. assign_sub 赋值操作，更新参数的值并返回 \n",
    "#                调用assign_sub之前，先调用tf.Variable定义变量w为可训练(可自更新)\n",
    "# w.assign_sub(w要自减的内容)\n",
    "w = tf.Variable(4)\n",
    "w.assign_sub(1)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e0d0a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]\n",
      " [5 4 3]\n",
      " [8 7 2]]\n",
      "tf.Tensor([3 3 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 15. tf.argmax(张量名,axis=操作轴) 返回张量沿指定维度最大索引\n",
    "\n",
    "test = np.array([[1,2,3],[2,3,4],[5,4,3],[8,7,2]])\n",
    "print(test)\n",
    "print(tf.argmax(test,axis=0))\n",
    "print(tf.argmax(test,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d0a8d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "x_data add index:\n",
      "      花萼长度  花萼宽度  花瓣长度  花瓣宽度\n",
      "0         5.1       3.5       1.4       0.2\n",
      "1         4.9       3.0       1.4       0.2\n",
      "2         4.7       3.2       1.3       0.2\n",
      "3         4.6       3.1       1.5       0.2\n",
      "4         5.0       3.6       1.4       0.2\n",
      "..        ...       ...       ...       ...\n",
      "145       6.7       3.0       5.2       2.3\n",
      "146       6.3       2.5       5.0       1.9\n",
      "147       6.5       3.0       5.2       2.0\n",
      "148       6.2       3.4       5.4       2.3\n",
      "149       5.9       3.0       5.1       1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "x_data add a colum: \n",
      "      花萼长度  花萼宽度  花瓣长度  花瓣宽度  类别\n",
      "0         5.1       3.5       1.4       0.2     0\n",
      "1         4.9       3.0       1.4       0.2     0\n",
      "2         4.7       3.2       1.3       0.2     0\n",
      "3         4.6       3.1       1.5       0.2     0\n",
      "4         5.0       3.6       1.4       0.2     0\n",
      "..        ...       ...       ...       ...   ...\n",
      "145       6.7       3.0       5.2       2.3     2\n",
      "146       6.3       2.5       5.0       1.9     2\n",
      "147       6.5       3.0       5.2       2.0     2\n",
      "148       6.2       3.4       5.4       2.3     2\n",
      "149       5.9       3.0       5.1       1.8     2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "\n",
    "x_data = DataFrame(x_data,columns=['花萼长度','花萼宽度','花瓣长度','花瓣宽度'])\n",
    "pd.set_option('display.unicode.east_asian_width',True)\n",
    "print('x_data add index:\\n',x_data)\n",
    "\n",
    "x_data['类别'] = y_data\n",
    "print('x_data add a colum: \\n',x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb9eff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 loss:0.2878302298486233\n",
      "test acc: 0.16666666666666666\n",
      "-----------------------\n",
      "Epoch:1 loss:0.26063910499215126\n",
      "test acc: 0.16666666666666666\n",
      "-----------------------\n",
      "Epoch:2 loss:0.23041951656341553\n",
      "test acc: 0.16666666666666666\n",
      "-----------------------\n",
      "Epoch:3 loss:0.21433782950043678\n",
      "test acc: 0.16666666666666666\n",
      "-----------------------\n",
      "Epoch:4 loss:0.2026178054511547\n",
      "test acc: 0.16666666666666666\n",
      "-----------------------\n",
      "Epoch:5 loss:0.19115766137838364\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:6 loss:0.18036313727498055\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:7 loss:0.170701764523983\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:8 loss:0.16233016550540924\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:9 loss:0.1551971398293972\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:10 loss:0.1491540763527155\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:11 loss:0.1440271269530058\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:12 loss:0.1396527010947466\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:13 loss:0.13589041493833065\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:14 loss:0.1326250322163105\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:15 loss:0.12976386584341526\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:16 loss:0.1272331103682518\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:17 loss:0.12497404403984547\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:18 loss:0.12293989397585392\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:19 loss:0.1210932508111\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:20 loss:0.1194040272384882\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:21 loss:0.11784786358475685\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:22 loss:0.11640497855842113\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:23 loss:0.11505910195410252\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:24 loss:0.11379685811698437\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:25 loss:0.11260714195668697\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:26 loss:0.11148069240152836\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:27 loss:0.11040973104536533\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:28 loss:0.10938768461346626\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:29 loss:0.10840900801122189\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:30 loss:0.10746895521879196\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:31 loss:0.10656346566975117\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:32 loss:0.10568905062973499\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:33 loss:0.10484269633889198\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:34 loss:0.10402180626988411\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:35 loss:0.10322409868240356\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:36 loss:0.10244759172201157\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:37 loss:0.10169055871665478\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:38 loss:0.10095145925879478\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:39 loss:0.10022895596921444\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:40 loss:0.09952188655734062\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:41 loss:0.09882915951311588\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:42 loss:0.09814986400306225\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:43 loss:0.09748315997421741\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:44 loss:0.09682829864323139\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:45 loss:0.0961846299469471\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:46 loss:0.09555153548717499\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:47 loss:0.09492850117385387\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:48 loss:0.09431503526866436\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:49 loss:0.09371068142354488\n",
      "test acc: 0.5333333333333333\n",
      "-----------------------\n",
      "Epoch:50 loss:0.0931150782853365\n",
      "test acc: 0.5666666666666667\n",
      "-----------------------\n",
      "Epoch:51 loss:0.0925278440117836\n",
      "test acc: 0.5666666666666667\n",
      "-----------------------\n",
      "Epoch:52 loss:0.09194865636527538\n",
      "test acc: 0.6\n",
      "-----------------------\n",
      "Epoch:53 loss:0.09137721173465252\n",
      "test acc: 0.6\n",
      "-----------------------\n",
      "Epoch:54 loss:0.09081327170133591\n",
      "test acc: 0.6\n",
      "-----------------------\n",
      "Epoch:55 loss:0.09025654383003712\n",
      "test acc: 0.6\n",
      "-----------------------\n",
      "Epoch:56 loss:0.08970683813095093\n",
      "test acc: 0.6\n",
      "-----------------------\n",
      "Epoch:57 loss:0.08916394039988518\n",
      "test acc: 0.6\n",
      "-----------------------\n",
      "Epoch:58 loss:0.0886276438832283\n",
      "test acc: 0.6\n",
      "-----------------------\n",
      "Epoch:59 loss:0.08809778653085232\n",
      "test acc: 0.6\n",
      "-----------------------\n",
      "Epoch:60 loss:0.08757419511675835\n",
      "test acc: 0.6\n",
      "-----------------------\n",
      "Epoch:61 loss:0.08705672807991505\n",
      "test acc: 0.6333333333333333\n",
      "-----------------------\n",
      "Epoch:62 loss:0.08654523268342018\n",
      "test acc: 0.6333333333333333\n",
      "-----------------------\n",
      "Epoch:63 loss:0.08603960834443569\n",
      "test acc: 0.6333333333333333\n",
      "-----------------------\n",
      "Epoch:64 loss:0.0855396818369627\n",
      "test acc: 0.6333333333333333\n",
      "-----------------------\n",
      "Epoch:65 loss:0.08504538238048553\n",
      "test acc: 0.6333333333333333\n",
      "-----------------------\n",
      "Epoch:66 loss:0.08455660752952099\n",
      "test acc: 0.6333333333333333\n",
      "-----------------------\n",
      "Epoch:67 loss:0.08407322689890862\n",
      "test acc: 0.6333333333333333\n",
      "-----------------------\n",
      "Epoch:68 loss:0.08359516970813274\n",
      "test acc: 0.6333333333333333\n",
      "-----------------------\n",
      "Epoch:69 loss:0.08312233351171017\n",
      "test acc: 0.6333333333333333\n",
      "-----------------------\n",
      "Epoch:70 loss:0.08265464194118977\n",
      "test acc: 0.6333333333333333\n",
      "-----------------------\n",
      "Epoch:71 loss:0.08219201304018497\n",
      "test acc: 0.6333333333333333\n",
      "-----------------------\n",
      "Epoch:72 loss:0.08173437230288982\n",
      "test acc: 0.6666666666666666\n",
      "-----------------------\n",
      "Epoch:73 loss:0.08128164894878864\n",
      "test acc: 0.6666666666666666\n",
      "-----------------------\n",
      "Epoch:74 loss:0.08083377033472061\n",
      "test acc: 0.6666666666666666\n",
      "-----------------------\n",
      "Epoch:75 loss:0.08039066381752491\n",
      "test acc: 0.7\n",
      "-----------------------\n",
      "Epoch:76 loss:0.0799522902816534\n",
      "test acc: 0.7\n",
      "-----------------------\n",
      "Epoch:77 loss:0.07951856590807438\n",
      "test acc: 0.7\n",
      "-----------------------\n",
      "Epoch:78 loss:0.0790894404053688\n",
      "test acc: 0.7\n",
      "-----------------------\n",
      "Epoch:79 loss:0.07866485230624676\n",
      "test acc: 0.7\n",
      "-----------------------\n",
      "Epoch:80 loss:0.07824474945664406\n",
      "test acc: 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch:81 loss:0.07782908715307713\n",
      "test acc: 0.7666666666666667\n",
      "-----------------------\n",
      "Epoch:82 loss:0.07741779088973999\n",
      "test acc: 0.7666666666666667\n",
      "-----------------------\n",
      "Epoch:83 loss:0.07701083086431026\n",
      "test acc: 0.7666666666666667\n",
      "-----------------------\n",
      "Epoch:84 loss:0.07660814467817545\n",
      "test acc: 0.7666666666666667\n",
      "-----------------------\n",
      "Epoch:85 loss:0.07620969880372286\n",
      "test acc: 0.7666666666666667\n",
      "-----------------------\n",
      "Epoch:86 loss:0.07581542804837227\n",
      "test acc: 0.7666666666666667\n",
      "-----------------------\n",
      "Epoch:87 loss:0.07542530167847872\n",
      "test acc: 0.7666666666666667\n",
      "-----------------------\n",
      "Epoch:88 loss:0.07503926567733288\n",
      "test acc: 0.7666666666666667\n",
      "-----------------------\n",
      "Epoch:89 loss:0.07465727720409632\n",
      "test acc: 0.7666666666666667\n",
      "-----------------------\n",
      "Epoch:90 loss:0.07427927758544683\n",
      "test acc: 0.8\n",
      "-----------------------\n",
      "Epoch:91 loss:0.07390525750815868\n",
      "test acc: 0.8\n",
      "-----------------------\n",
      "Epoch:92 loss:0.07353513967245817\n",
      "test acc: 0.8\n",
      "-----------------------\n",
      "Epoch:93 loss:0.07316889334470034\n",
      "test acc: 0.8\n",
      "-----------------------\n",
      "Epoch:94 loss:0.07280648499727249\n",
      "test acc: 0.8\n",
      "-----------------------\n",
      "Epoch:95 loss:0.0724478717893362\n",
      "test acc: 0.8333333333333334\n",
      "-----------------------\n",
      "Epoch:96 loss:0.072092997841537\n",
      "test acc: 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch:97 loss:0.07174183707684278\n",
      "test acc: 0.8666666666666667\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:98 loss:0.07139434944838285\n",
      "test acc: 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch:99 loss:0.07105048932135105\n",
      "test acc: 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch:100 loss:0.07071020640432835\n",
      "test acc: 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch:101 loss:0.07037349510937929\n",
      "test acc: 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch:102 loss:0.07004028651863337\n",
      "test acc: 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch:103 loss:0.06971054803580046\n",
      "test acc: 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch:104 loss:0.0693842489272356\n",
      "test acc: 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch:105 loss:0.06906134821474552\n",
      "test acc: 0.9\n",
      "-----------------------\n",
      "Epoch:106 loss:0.0687418133020401\n",
      "test acc: 0.9\n",
      "-----------------------\n",
      "Epoch:107 loss:0.06842559296637774\n",
      "test acc: 0.9\n",
      "-----------------------\n",
      "Epoch:108 loss:0.06811266113072634\n",
      "test acc: 0.9\n",
      "-----------------------\n",
      "Epoch:109 loss:0.06780297867953777\n",
      "test acc: 0.9\n",
      "-----------------------\n",
      "Epoch:110 loss:0.0674965139478445\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:111 loss:0.06719322688877583\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:112 loss:0.06689307186752558\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:113 loss:0.06659602466970682\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:114 loss:0.06630205642431974\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:115 loss:0.06601111497730017\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:116 loss:0.06572317518293858\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:117 loss:0.06543820630759001\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:118 loss:0.06515616923570633\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:119 loss:0.06487702671438456\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:120 loss:0.0646007526665926\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:121 loss:0.06432731356471777\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:122 loss:0.06405666563659906\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:123 loss:0.06378878001123667\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:124 loss:0.06352363526821136\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:125 loss:0.06326119042932987\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:126 loss:0.06300141476094723\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:127 loss:0.06274427194148302\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:128 loss:0.06248973961919546\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:129 loss:0.062237770296633244\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:130 loss:0.06198835838586092\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:131 loss:0.06174145080149174\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:132 loss:0.06149702612310648\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:133 loss:0.061255055479705334\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:134 loss:0.06101551465690136\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:135 loss:0.06077836733311415\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:136 loss:0.06054357718676329\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:137 loss:0.06031113024801016\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:138 loss:0.0600809957832098\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:139 loss:0.05985313653945923\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:140 loss:0.0596275245770812\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:141 loss:0.059404145926237106\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:142 loss:0.059182957746088505\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:143 loss:0.058963948860764503\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:144 loss:0.05874708015471697\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:145 loss:0.05853232368826866\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:146 loss:0.058319658041000366\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:147 loss:0.058109065517783165\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:148 loss:0.05790051817893982\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:149 loss:0.0576939731836319\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:150 loss:0.0574894305318594\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:151 loss:0.057286848314106464\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:152 loss:0.05708620883524418\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:153 loss:0.0568874878808856\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:154 loss:0.05669066030532122\n",
      "test acc: 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch:155 loss:0.056495705619454384\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:156 loss:0.05630260519683361\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:157 loss:0.05611132737249136\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:158 loss:0.05592184327542782\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:159 loss:0.05573413707315922\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:160 loss:0.05554819945245981\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:161 loss:0.05536399967968464\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:162 loss:0.05518151726573706\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:163 loss:0.0550007214769721\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:164 loss:0.05482161231338978\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:165 loss:0.054644144140183926\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:166 loss:0.05446830950677395\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:167 loss:0.0542940990999341\n",
      "test acc: 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch:168 loss:0.054121469147503376\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:169 loss:0.05395041964948177\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:170 loss:0.05378092173486948\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:171 loss:0.05361295863986015\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:172 loss:0.05344652011990547\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:173 loss:0.05328156799077988\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:174 loss:0.053118099458515644\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:175 loss:0.052956090308725834\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:176 loss:0.052795534953475\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:177 loss:0.05263638962060213\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:178 loss:0.0524786664173007\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:179 loss:0.052322336472570896\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:180 loss:0.05216736998409033\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:181 loss:0.052013770677149296\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:182 loss:0.05186151992529631\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:183 loss:0.051710592582821846\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:184 loss:0.05156096536666155\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:185 loss:0.05141263920813799\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:186 loss:0.05126559920608997\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:187 loss:0.051119815558195114\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:188 loss:0.050975278951227665\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:189 loss:0.05083198007196188\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:190 loss:0.05068989936262369\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:191 loss:0.05054901819676161\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:192 loss:0.05040934029966593\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:193 loss:0.05027082376182079\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:194 loss:0.05013348441570997\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:195 loss:0.04999728035181761\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:196 loss:0.049862215295434\n",
      "test acc: 1.0\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:197 loss:0.049728275276720524\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:198 loss:0.049595437943935394\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:199 loss:0.04946370515972376\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:200 loss:0.04933304898440838\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:201 loss:0.04920346476137638\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:202 loss:0.04907494131475687\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:203 loss:0.04894745443016291\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:204 loss:0.04882101248949766\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:205 loss:0.04869559034705162\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:206 loss:0.04857117123901844\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:207 loss:0.04844775516539812\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:208 loss:0.048325324431061745\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:209 loss:0.04820386879146099\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:210 loss:0.04808338452130556\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:211 loss:0.04796383995562792\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:212 loss:0.047845251858234406\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:213 loss:0.04772758297622204\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:214 loss:0.047610845416784286\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:215 loss:0.047495017759501934\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:216 loss:0.047380086965858936\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:217 loss:0.047266051173210144\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:218 loss:0.04715289641171694\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:219 loss:0.0470406087115407\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:220 loss:0.04692918621003628\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:221 loss:0.04681861586868763\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:222 loss:0.046708881855010986\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:223 loss:0.04659998323768377\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:224 loss:0.04649190790951252\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:225 loss:0.046384647488594055\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:226 loss:0.04627820011228323\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:227 loss:0.04617254249751568\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:228 loss:0.04606767185032368\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:229 loss:0.04596358444541693\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:230 loss:0.04586027003824711\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:231 loss:0.045757707208395004\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:232 loss:0.04565591178834438\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:233 loss:0.04555485676974058\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:234 loss:0.04545453563332558\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:235 loss:0.04535495117306709\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:236 loss:0.04525609128177166\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:237 loss:0.04515793826431036\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:238 loss:0.04506050422787666\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:239 loss:0.0449637733399868\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:240 loss:0.04486771672964096\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:241 loss:0.044772359542548656\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:242 loss:0.0446776757016778\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:243 loss:0.04458365961909294\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:244 loss:0.04449030850082636\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:245 loss:0.04439761210232973\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:246 loss:0.04430557321757078\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:247 loss:0.04421417135745287\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:248 loss:0.04412340745329857\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:249 loss:0.04403327684849501\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:250 loss:0.043943761847913265\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:251 loss:0.043854876421391964\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:252 loss:0.04376659914851189\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:253 loss:0.04367891512811184\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:254 loss:0.043591844849288464\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:255 loss:0.04350535571575165\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:256 loss:0.043419456109404564\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:257 loss:0.04333414323627949\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:258 loss:0.0432493994012475\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:259 loss:0.04316522181034088\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:260 loss:0.04308160953223705\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:261 loss:0.04299855977296829\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:262 loss:0.04291606601327658\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:263 loss:0.042834105901420116\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:264 loss:0.04275268781930208\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:265 loss:0.04267180897295475\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:266 loss:0.0425914628431201\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:267 loss:0.04251163452863693\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:268 loss:0.04243233986198902\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:269 loss:0.0423535518348217\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:270 loss:0.04227527976036072\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:271 loss:0.04219750966876745\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:272 loss:0.04212022479623556\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:273 loss:0.042043449357151985\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:274 loss:0.0419671731069684\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:275 loss:0.041891373693943024\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:276 loss:0.041816056706011295\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:277 loss:0.041741215623915195\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:278 loss:0.041666838340461254\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:279 loss:0.041592944879084826\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:280 loss:0.041519496124237776\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:281 loss:0.041446521412581205\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:282 loss:0.04137399699538946\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:283 loss:0.04130192939192057\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:284 loss:0.04123029066249728\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:285 loss:0.04115910967811942\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:286 loss:0.04108834872022271\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:287 loss:0.04101803619414568\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:288 loss:0.04094815021380782\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:289 loss:0.0408786884509027\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:290 loss:0.04080964857712388\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:291 loss:0.0407410329207778\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:292 loss:0.04067282332107425\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:293 loss:0.04060502024367452\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:294 loss:0.04053763626143336\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:295 loss:0.04047064622864127\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:296 loss:0.04040406132116914\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:297 loss:0.04033786850050092\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:298 loss:0.04027207242324948\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:299 loss:0.04020665679126978\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:300 loss:0.04014162579551339\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:301 loss:0.04007698455825448\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:302 loss:0.04001271491870284\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:303 loss:0.03994882432743907\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:304 loss:0.0398852969519794\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:305 loss:0.03982214583083987\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:306 loss:0.03975935513153672\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:307 loss:0.03969692857936025\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:308 loss:0.03963486570864916\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:309 loss:0.039573149755597115\n",
      "test acc: 1.0\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:310 loss:0.0395117849111557\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:311 loss:0.03945077722892165\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:312 loss:0.039390109945088625\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:313 loss:0.03932978492230177\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:314 loss:0.03926981007680297\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:315 loss:0.03921017283573747\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:316 loss:0.03915085690096021\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:317 loss:0.039091880433261395\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:318 loss:0.03903323179110885\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:319 loss:0.03897490864619613\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:320 loss:0.038916910998523235\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:321 loss:0.03885922580957413\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:322 loss:0.03880185913294554\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:323 loss:0.03874481841921806\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:324 loss:0.03868808317929506\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:325 loss:0.03863165760412812\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:326 loss:0.038575537502765656\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:327 loss:0.0385197214782238\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:328 loss:0.038464209996163845\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:329 loss:0.03840899607166648\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:330 loss:0.03835408482700586\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:331 loss:0.03829945903271437\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:332 loss:0.038245131727308035\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:333 loss:0.03819109592586756\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:334 loss:0.038137342780828476\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:335 loss:0.03808388113975525\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:336 loss:0.03803069982677698\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:337 loss:0.03797779371961951\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:338 loss:0.03792517213150859\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:339 loss:0.03787281736731529\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:340 loss:0.037820748053491116\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:341 loss:0.03776894137263298\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:342 loss:0.03771740756928921\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:343 loss:0.03766613733023405\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:344 loss:0.03761513717472553\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:345 loss:0.037564401514828205\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:346 loss:0.03751392103731632\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:347 loss:0.037463700864464045\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:348 loss:0.03741373494267464\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:349 loss:0.037364032585173845\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:350 loss:0.03731456957757473\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:351 loss:0.037265369202941656\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:352 loss:0.03721641609445214\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:353 loss:0.03716770512983203\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:354 loss:0.03711923863738775\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:355 loss:0.037071018014103174\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:356 loss:0.03702303720638156\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:357 loss:0.03697529900819063\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:358 loss:0.036927795968949795\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:359 loss:0.0368805299513042\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:360 loss:0.03683349443599582\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:361 loss:0.03678669407963753\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:362 loss:0.03674011956900358\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:363 loss:0.03669377788901329\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:364 loss:0.03664765786379576\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:365 loss:0.03660177253186703\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:366 loss:0.03655609814450145\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:367 loss:0.03651065472513437\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:368 loss:0.03646542690694332\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:369 loss:0.03642042726278305\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:370 loss:0.036375627387315035\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:371 loss:0.036331047769635916\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:372 loss:0.03628669260069728\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:373 loss:0.03624253813177347\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:374 loss:0.03619860019534826\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:375 loss:0.03615486854687333\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:376 loss:0.03611133759841323\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:377 loss:0.036068023182451725\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:378 loss:0.03602490900084376\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:379 loss:0.03598200064152479\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:380 loss:0.03593928599730134\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:381 loss:0.035896781366318464\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:382 loss:0.03585446672514081\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:383 loss:0.03581235883757472\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:384 loss:0.03577042883262038\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:385 loss:0.03572870930656791\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:386 loss:0.03568717185407877\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:387 loss:0.03564583137631416\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:388 loss:0.035604679491370916\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:389 loss:0.03556371480226517\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:390 loss:0.035522935912013054\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:391 loss:0.035482353530824184\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:392 loss:0.03544194856658578\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:393 loss:0.035401725210249424\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:394 loss:0.035361690912395716\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:395 loss:0.03532182797789574\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:396 loss:0.03528214944526553\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:397 loss:0.035242646001279354\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:398 loss:0.035203317645937204\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:399 loss:0.035164165776222944\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:400 loss:0.035125191789120436\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:401 loss:0.03508639335632324\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:402 loss:0.03504776209592819\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:403 loss:0.035009306855499744\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:404 loss:0.034971016459167004\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:405 loss:0.03493289602920413\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:406 loss:0.034894939977675676\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:407 loss:0.03485716413706541\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:408 loss:0.03481954149901867\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:409 loss:0.03478208323940635\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:410 loss:0.034744786098599434\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:411 loss:0.03470765007659793\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:412 loss:0.034670681692659855\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:413 loss:0.034633869770914316\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:414 loss:0.03459721617400646\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:415 loss:0.03456071671098471\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:416 loss:0.03452438162639737\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:417 loss:0.034488195553421974\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:418 loss:0.03445217292755842\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:419 loss:0.0344162886030972\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:420 loss:0.034380558878183365\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:421 loss:0.03434498934075236\n",
      "test acc: 1.0\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:422 loss:0.03430956695228815\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:423 loss:0.03427429124712944\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:424 loss:0.03423916408792138\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:425 loss:0.03420418594032526\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:426 loss:0.03416935168206692\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:427 loss:0.03413466503843665\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:428 loss:0.03410012507811189\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:429 loss:0.03406573086977005\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:430 loss:0.03403147542849183\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:431 loss:0.03399735316634178\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:432 loss:0.03396338410675526\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:433 loss:0.0339295482262969\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:434 loss:0.03389585530385375\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:435 loss:0.03386229882016778\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:436 loss:0.033828872721642256\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:437 loss:0.0337955909781158\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:438 loss:0.033762449864298105\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:439 loss:0.033729436341673136\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:440 loss:0.03369655553251505\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:441 loss:0.03366380510851741\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:442 loss:0.033631197176873684\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:443 loss:0.03359871171414852\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:444 loss:0.033566360361874104\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:445 loss:0.03353413846343756\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:446 loss:0.0335020343773067\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:447 loss:0.033470076974481344\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:448 loss:0.033438230864703655\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:449 loss:0.033406516537070274\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:450 loss:0.03337493073195219\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:451 loss:0.03334346553310752\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:452 loss:0.03331213118508458\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:453 loss:0.03328090626746416\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:454 loss:0.03324982337653637\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:455 loss:0.03321884898468852\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:456 loss:0.03318800078704953\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:457 loss:0.03315726388245821\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:458 loss:0.03312665643170476\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:459 loss:0.03309616353362799\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:460 loss:0.03306578751653433\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:461 loss:0.03303552931174636\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:462 loss:0.03300539031624794\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:463 loss:0.0329753695987165\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:464 loss:0.03294545318931341\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:465 loss:0.03291566111147404\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:466 loss:0.03288597846403718\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:467 loss:0.032856410369277\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:468 loss:0.03282695356756449\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:469 loss:0.03279761364683509\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:470 loss:0.03276837803423405\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:471 loss:0.0327392527833581\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:472 loss:0.03271024348214269\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:473 loss:0.03268134593963623\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:474 loss:0.03265254572033882\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:475 loss:0.03262385819107294\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:476 loss:0.03259527962654829\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:477 loss:0.03256680630147457\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:478 loss:0.03253844752907753\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:479 loss:0.03251017117872834\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:480 loss:0.03248202195391059\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:481 loss:0.03245396679267287\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:482 loss:0.03242602199316025\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:483 loss:0.03239817498251796\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:484 loss:0.03237043647095561\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:485 loss:0.032342785969376564\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:486 loss:0.03231524722650647\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:487 loss:0.03228780720382929\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:488 loss:0.032260467763990164\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:489 loss:0.032233224250376225\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:490 loss:0.03220608504489064\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:491 loss:0.032179034780710936\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:492 loss:0.032152094412595034\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:493 loss:0.03212524112313986\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:494 loss:0.032098493073135614\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:495 loss:0.032071834430098534\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:496 loss:0.03204527124762535\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:497 loss:0.032018810510635376\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:498 loss:0.031992433592677116\n",
      "test acc: 1.0\n",
      "-----------------------\n",
      "Epoch:499 loss:0.031966155394911766\n",
      "test acc: 1.0\n",
      "-----------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]\n",
    "\n",
    "x_train = tf.cast(x_train,tf.float32)\n",
    "x_test = tf.cast(x_test,tf.float32)\n",
    "\n",
    "# 分批\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(30)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(30)\n",
    "\n",
    "# 生成神经网络的参数，4个输入特征，所以输入层为4个输入节点，因为3分类，所以输出层为3个神经元\n",
    "# 使用seed使每次生成的随机数相同（在现实使用时不写）\n",
    "# stddev 标准差\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4,3],stddev=0.1,seed=1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3],stddev=0.1,seed=1))\n",
    "\n",
    "lr = 0.1 #学习率\n",
    "train_loss_results = []\n",
    "test_acc = []\n",
    "epoch = 500\n",
    "loss_all = 0\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    for step,(x_train,y_train) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = tf.matmul(x_train,w1)+b1\n",
    "            y = tf.nn.softmax(y)\n",
    "            y_ = tf.one_hot(y_train,depth=3)\n",
    "            loss = tf.reduce_mean(tf.square(y_-y))\n",
    "            loss_all += loss.numpy()\n",
    "            \n",
    "        grads = tape.gradient(loss,[w1,b1])\n",
    "        \n",
    "        w1.assign_sub(lr*grads[0])\n",
    "        b1.assign_sub(lr*grads[1])\n",
    "        \n",
    "    print(\"Epoch:{} loss:{}\".format(epoch,loss_all/4))\n",
    "    train_loss_results.append(loss_all/4)\n",
    "    loss_all = 0\n",
    "    \n",
    "    \n",
    "    total_correct,total_number= 0,0\n",
    "    for x_test,y_test in test_db:\n",
    "        y = tf.matmul(x_test,w1)+b1\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y,axis=1)\n",
    "        \n",
    "        correct = tf.cast(tf.equal(pred,y_test),dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        \n",
    "        total_correct += int(correct)\n",
    "        \n",
    "        total_number += x_test.shape[0]\n",
    "        \n",
    "    acc = total_correct/total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"test acc:\",acc)\n",
    "    print(\"-----------------------\")\n",
    "    \n",
    "plt.title(\"Loss Funtion curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_results,label=\"$Loss$\") #\n",
    "plt.legend() #画出曲线\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Acc curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.plot(test_acc,label=\"$Accuracy$\") #\n",
    "plt.legend() #画出曲线\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d7c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
